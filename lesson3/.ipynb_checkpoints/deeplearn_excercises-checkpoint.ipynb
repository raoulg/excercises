{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import fashion_mnist, mnist\n",
    "import datetime\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input, Dropout, BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "!rm -rf logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create networks with different shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model with:\n",
    "- one `Input` layer for datashapes (batch x 10), so 10 features per observation\n",
    "- one `Dense` deep layer of 30 units\n",
    "- one `Dense` outputlayer, with an output of (batch x 1), so 1 number.\n",
    "- thus, a total of three layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2ba66f3c3b0c70c3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert (model._input_layers[0].input_shape[0][1] == 10) \n",
    "assert (len(model._input_layers) == 1) \n",
    "assert (len(model.layers) == 3) \n",
    "assert (model.layers[1].output_shape[1] == 30) \n",
    "assert (len(model._output_layers) == 1)\n",
    "assert (model._output_layers[0].output_shape[1] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model that:\n",
    "- starts with two `Input` layers (`inputa` and `inputb`), both for data with shape (batch x 10)\n",
    "- input `a` feeds into a `Dense` layer with 50 units\n",
    "- the output of that Dense layer is concatenated with input `b`\n",
    "- So, the concatenated layer should output a layer of shape (batch x 60), because we concatenate 50 + 10\n",
    "- the concatenated layers are fed into a `Dense` layer with a single unit, which is the output of the model.\n",
    "- we have a total of 5 layers (2 input, 1 Dense, 1 concat, 1 Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f327dffd4c99b9ce",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(model._input_layers) == 2 \n",
    "assert (model._input_layers[0].input_shape[0][1] == 10 & model._input_layers[1].input_shape[0][1] == 10) \n",
    "assert model.layers[1].output_shape[1] == 50\n",
    "assert model.layers[3].output_shape[0][1] == 60\n",
    "assert model.layers[4].output_shape[1] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, add a double output to the above model. The output of the first `Dense` layer with 50 units should also be an output, together with the output of the second `Dense` layer with one unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-081fc797635866b3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(model._input_layers) == 2 \n",
    "assert (model._input_layers[0].input_shape[0][1] == 10 & model._input_layers[1].input_shape[0][1] == 10) \n",
    "assert model.layers[1].output_shape[1] == 50\n",
    "assert model.layers[3].output_shape[0][1] == 60\n",
    "assert model.layers[4].output_shape[1] == 1\n",
    "assert len(model._output_layers) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a model for MNIST\n",
    "## prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the mnist dataset and make a train-test split. Make sure the random_state and size does not change, because that will influence the `assert` tests later on.\n",
    "\n",
    "Note that in a real life setting you should ONLY use `random_state` if you want to guarantee that the split comes out exactly the same every time you make it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(X_train, y_train), (X_valid, y_valid) = fashion_mnist.load_data()\n",
    "(X_train, y_train), (X_valid, y_valid) = mnist.load_data()\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_valid.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2 #let's have a look at case 25. You can change this to have a look at others\n",
    "digit = X_train[idx]\n",
    "plt.imshow(digit, cmap='binary')\n",
    "y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the y are categories, ranging from 0 to 9.\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we scale the data, simply between [0,1]\n",
    "X_train = X_train / 255.\n",
    "X_valid = X_valid / 255.\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we reshape\n",
    "X_trainr = X_train.reshape(X_train.shape[0], -1)\n",
    "X_testr = X_test.reshape(X_test.shape[0], -1)\n",
    "X_trainr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reshaping, we have size (batch x features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usefull for plotting heatmaps of a confusion matrix\n",
    "import seaborn as sns\n",
    "def cfm_heatmap(cfm, figsize = (8,8), scale = None, vmin=None, vmax=None):\n",
    "    \"\"\"\n",
    "    figsize: tuple, default (8,8)\n",
    "    scale: string. The direction over which the numbers are scaled. Either None, 'total', 'rowwise' or 'colwise'\n",
    "    \"\"\"\n",
    "    if (scale == 'total'):\n",
    "        cfm_norm = cfm / np.sum(cfm)\n",
    "    elif (scale == 'rowwise'):\n",
    "        cfm_norm = cfm / np.sum(cfm, axis=1, keepdims=True)\n",
    "    elif (scale == 'colwise'):\n",
    "        cfm_norm = cfm / np.sum(cfm, axis=0, keepdims=True)\n",
    "    else:\n",
    "        cfm_norm = cfm\n",
    "    plt.figure(figsize=figsize)\n",
    "    plot = sns.heatmap(cfm_norm, annot = cfm_norm, vmin=vmin, vmax=vmax)\n",
    "    plot.set(xlabel = 'Predicted', ylabel = 'Target')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a basic classifier. The simplest is a `SGDClassifier`. Make one, fit, predict and make a confusion matrix. Tip: speed it up with `n_jobs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And test the accuracy. This is the baseline you want to improve on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9adc9c8a00f4d581",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert accuracy_score(y_test, yhat) > 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a Sequential model. Try some variations with amounts of layers and units. \n",
    "Experiment with the following things:\n",
    "\n",
    "- different amounts of layers\n",
    "- different amounts of units in every layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "model = Sequential([\n",
    "\n",
    "    # your code here\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), callbacks=[early_stop], verbose = 0)\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cb9a653014ddf143",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# you should be able to get above 96% with a bit of trying\n",
    "assert model.evaluate(X_test, y_test)[1] > 0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a hypermodel.\n",
    "\n",
    "define ranges for\n",
    "- amounts of units (at least between 128 and 320)\n",
    "- amounts of layers (range at least between 1 and 6) with a forloop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    input = Input(shape = [28,28])\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    output = Dense(10, activation='softmax')(x)\n",
    "    model = Model(inputs = [input], outputs = [output])\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kerastuner as kt\n",
    "# cleaning up folders from old runs\n",
    "!rm -rf ktuner/\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=3,\n",
    "    directory='ktuner',\n",
    "    project_name='mnist'\n",
    ")\n",
    "tuner.search(X_train, y_train, epochs = 10, validation_data = (X_valid, y_valid), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we obtain the best model, and fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "print(best_hps.values)\n",
    "model = tuner.get_best_models()[0]\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[early_stop], verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to get up to 97.5, even above 98."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-03d4c102df2a19d1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert model.evaluate(X_test, y_test)[1] > 0.975"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
