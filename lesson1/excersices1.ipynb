{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANCHORMEN ACADEMY\n",
    "# EXCERCISES 1 - SOLUTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.11.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading libraries, some settings for plots\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [8, 8]\n",
    "sns.__version__ #should be >= 0.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv('~/shared/cancer_data_uncleaned.csv', index_col=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4270d212c2009160",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Explore the data. Think of functions like `describe` or `info`. Take a good look at the columns, and clean up things that are obviously weird or wrong. \n",
    "\n",
    "1. Are there NaN's? If you find complete columns or rows with NaN's, remove them.\n",
    "2. Check for exceptional outliers, that are most likely a mistake. To do so, scale the data to compare things in one boxplot, and after scaling check for the biggest outlier. If you find observations that are obviously an error, remove the complete row. Check your result visually, eg with boxplots or a pairplot.\n",
    "\n",
    "What is the immpact of huge outliers on scaling? For your final dataset, would you want to apply scaling before, or after removing outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9dc5b082dbb78dd8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# after removing NaN's and obvious errors, this test should pass on the unscaled data.\n",
    "X = df.drop('diagnosis', axis=1)\n",
    "y = df['diagnosis']\n",
    "\n",
    "\n",
    "# cell for testing, do not modify\n",
    "assert (np.sum(np.sum(X)) + np.sum(X.shape)) == 1057073.4596356002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into a train and test set. Use a `test_size` of 0.3. \n",
    "\n",
    "If you use `random_state` 4, you will get comparable results with the solutions (plus this might make a difference in your results on the testset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import train_test_split\n",
    "X_train, X_test, y_train, y_test = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usefull for plotting the heatmap of the combination of the two parameters\n",
    "def gridsearch_heatmap(gridresults, param_grid, vmin = None, vmax = None):\n",
    "    # vmin and vmax change the contrast of the color scheme\n",
    "    idx, col = ['param_' + [*param_grid.keys()][i] for i in range(2)] \n",
    "    #idx, col = ['param_' + key for key in param_grid.keys()] \n",
    "    pivoted = pd.pivot_table(pd.DataFrame(gridresults.cv_results_),\n",
    "                            values = 'mean_test_score',\n",
    "                            index = idx,\n",
    "                            columns = col)\n",
    "    pivoted.columns = [\"{:.4f}\".format(x) for x in pivoted.columns]\n",
    "    #annot = pivoted.round(4)\n",
    "    sns.heatmap(pivoted, vmin = vmin, vmax = vmax, annot = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline with a `StandardScaler` and a `SVC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Build pipeline\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        # your code here\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a parameter grid for the svm in the pipeline. Start with logarithmic steps for the values, ranging from 0.0001 till 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1, 1, 10]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use list comprehension to generate steps, eg:\n",
    "[10**x for x in range(-1,2)]\n",
    "# this is easier to modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter grid\n",
    "param_grid = {'svm__C': [ # your code here],\n",
    "              'svm__gamma': [ # your code here]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a GridSearchCV with the pipe. Fit on the trainset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch = # your code here\n",
    "\n",
    "# Fit GridSearchCV\n",
    "gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results with the `gridsearch_heatmap` function. Explain (2 sentences) what you are looking at, and what it means for the model. Explain (2 sentences) based on this, what is the parameter range you would pick for the model and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_heatmap(gridsearch, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see? How do you explain this? Now, zoom in on the parameters. Take the area with the best heatspot and increase the granularity. Iterate until you are satisfied. You should be able to achieve at leats 0.95 accuracy on the testset, but you can even get above 0.98."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter grid\n",
    "param_grid = # your code here\n",
    "# create pipe\n",
    "gridsearch = # your code here\n",
    "# fit the model\n",
    "gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the imagesize, so you can read the numbers, and visualize again.\n",
    "# Set vmnin and vmax on the heatmap function for optimal contrast.\n",
    "plt.rcParams['figure.figsize'] = [15, 15]\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the best scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d84ed5864841c53b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# cell for testing, do not modify\n",
    "assert gridsearch.score(X_test, y_test) > 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you figure out how you can manually overwrite variables on an existing pipe (other then the `best_score_` from the gridsearch)?\n",
    "\n",
    "Hint: look at the `set_params` method on the pipe. Fit and score the pipe with `C=11` and `gamma=0.0163` , without running a gridsearch.\n",
    "\n",
    " Why would you want to do that, instead of just using `best_score` from the gridsearch? At what point in the process is this a smart move? Based on what would you pick another value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-adb31648a093aa44",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### do not modify\n",
    "model = [*pipe.named_steps][-1]\n",
    "params = [pipe.named_steps[model].get_params()[key] for key in ['C', 'gamma']]\n",
    "assert params == [11, 0.0163]\n",
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you figure out how to add another kernel to the pipe? Read the documentation from sklearn on SVC to find out your options. Show you can add a 'sigmoid' kernel to the svm in the gridsearch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = # your code here\n",
    "\n",
    "gridsearch = # your code here\n",
    "\n",
    "# Fit GridSearchCV\n",
    "\n",
    "gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6966012a8edaeba2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# cell for testing, do not modify or remove\n",
    "### BEGIN HIDDEN TESTS\n",
    "model = [*pipe.named_steps][-1]\n",
    "assert pipe.named_steps[model].kernel =='rbf' and gridsearch.best_estimator_.named_steps[model].kernel == 'sigmoid'\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the outcomes.\n",
    "# print the best outcomes.\n",
    "# iterate as long as you think it can improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you figure out how to switch the SVC for a `RandomForestClassifier` in the pipe? Read the documentation from `sklearn`, and do a gridsearch on different numbers of `n_estimators`. Take a range you suspect to be interesting based on the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Build pipeline\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        # your code here\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_grid = # your code here\n",
    "\n",
    "gridsearch = GridSearchCV(pipe, param_grid=param_grid, cv=3)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8e4702c12282d555",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# code for testing, do not modify\n",
    "\n",
    "\n",
    "import sklearn\n",
    "model = [*pipe.named_steps][-1]\n",
    "assert type(pipe.named_steps[model]) == sklearn.ensemble._forest.RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print the best outcomes\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare to the SVC? Can you plot the result for the different values of `n_estimators`? How does this compare to the `SVC`? How certain are you of the results on onseen data, based on these results? Which one would you pick, just based on these results?\n",
    "\n",
    "Hint: look at the `.cv_results_` object in the gridsearch. Decide how to plot this so you can visualize what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
