{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "plt.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and make a train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-33.649387</td>\n",
       "      <td>0.485653</td>\n",
       "      <td>0.901637</td>\n",
       "      <td>-1.212651</td>\n",
       "      <td>0.139052</td>\n",
       "      <td>0.709391</td>\n",
       "      <td>-0.994617</td>\n",
       "      <td>0.282911</td>\n",
       "      <td>-2.175066</td>\n",
       "      <td>0.853641</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.287330</td>\n",
       "      <td>0.834672</td>\n",
       "      <td>-0.692276</td>\n",
       "      <td>0.627274</td>\n",
       "      <td>1.149234</td>\n",
       "      <td>0.464482</td>\n",
       "      <td>1.593337</td>\n",
       "      <td>0.274752</td>\n",
       "      <td>2.152221</td>\n",
       "      <td>0.006960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120.201493</td>\n",
       "      <td>-0.893088</td>\n",
       "      <td>-0.071643</td>\n",
       "      <td>0.009755</td>\n",
       "      <td>1.347144</td>\n",
       "      <td>0.379104</td>\n",
       "      <td>0.093540</td>\n",
       "      <td>0.585669</td>\n",
       "      <td>-1.109070</td>\n",
       "      <td>-0.617355</td>\n",
       "      <td>...</td>\n",
       "      <td>1.024032</td>\n",
       "      <td>1.248742</td>\n",
       "      <td>0.383929</td>\n",
       "      <td>0.456798</td>\n",
       "      <td>0.708521</td>\n",
       "      <td>-1.075047</td>\n",
       "      <td>0.428324</td>\n",
       "      <td>-0.803252</td>\n",
       "      <td>-0.114866</td>\n",
       "      <td>0.217488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-141.708704</td>\n",
       "      <td>0.456587</td>\n",
       "      <td>-0.472096</td>\n",
       "      <td>1.606499</td>\n",
       "      <td>1.367399</td>\n",
       "      <td>-0.073593</td>\n",
       "      <td>0.981937</td>\n",
       "      <td>-0.002895</td>\n",
       "      <td>0.250887</td>\n",
       "      <td>0.215370</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.540053</td>\n",
       "      <td>-1.386105</td>\n",
       "      <td>-1.124981</td>\n",
       "      <td>-0.012272</td>\n",
       "      <td>-2.196037</td>\n",
       "      <td>-0.213129</td>\n",
       "      <td>1.151220</td>\n",
       "      <td>0.885248</td>\n",
       "      <td>1.226929</td>\n",
       "      <td>-0.100698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-184.095636</td>\n",
       "      <td>-0.765121</td>\n",
       "      <td>-0.938719</td>\n",
       "      <td>-0.854190</td>\n",
       "      <td>0.131660</td>\n",
       "      <td>-0.414848</td>\n",
       "      <td>-0.008027</td>\n",
       "      <td>-0.988914</td>\n",
       "      <td>-1.434419</td>\n",
       "      <td>0.310836</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.893789</td>\n",
       "      <td>0.059948</td>\n",
       "      <td>-0.887020</td>\n",
       "      <td>-0.700581</td>\n",
       "      <td>-0.671862</td>\n",
       "      <td>1.461510</td>\n",
       "      <td>2.365019</td>\n",
       "      <td>-1.583156</td>\n",
       "      <td>-1.046293</td>\n",
       "      <td>-1.532051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>246.179468</td>\n",
       "      <td>-0.382082</td>\n",
       "      <td>0.026456</td>\n",
       "      <td>2.268877</td>\n",
       "      <td>0.527533</td>\n",
       "      <td>-0.053195</td>\n",
       "      <td>1.566953</td>\n",
       "      <td>-1.114428</td>\n",
       "      <td>-0.545163</td>\n",
       "      <td>0.872815</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.984395</td>\n",
       "      <td>0.361132</td>\n",
       "      <td>2.491734</td>\n",
       "      <td>-1.471508</td>\n",
       "      <td>-1.187280</td>\n",
       "      <td>-1.700168</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>-0.309938</td>\n",
       "      <td>0.253091</td>\n",
       "      <td>-0.076393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>-47.645625</td>\n",
       "      <td>0.270316</td>\n",
       "      <td>-0.414474</td>\n",
       "      <td>-0.998558</td>\n",
       "      <td>-0.315235</td>\n",
       "      <td>-0.528897</td>\n",
       "      <td>0.980720</td>\n",
       "      <td>0.484330</td>\n",
       "      <td>-0.183577</td>\n",
       "      <td>-0.929874</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.110900</td>\n",
       "      <td>-0.214779</td>\n",
       "      <td>0.840373</td>\n",
       "      <td>0.376225</td>\n",
       "      <td>-2.229612</td>\n",
       "      <td>-0.415535</td>\n",
       "      <td>0.857946</td>\n",
       "      <td>-0.408338</td>\n",
       "      <td>0.738022</td>\n",
       "      <td>0.470555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>-97.823612</td>\n",
       "      <td>-1.860027</td>\n",
       "      <td>0.694745</td>\n",
       "      <td>0.150319</td>\n",
       "      <td>-0.272093</td>\n",
       "      <td>0.974670</td>\n",
       "      <td>-0.352025</td>\n",
       "      <td>-0.096521</td>\n",
       "      <td>0.307004</td>\n",
       "      <td>-0.686755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536013</td>\n",
       "      <td>0.226708</td>\n",
       "      <td>0.220630</td>\n",
       "      <td>-0.734824</td>\n",
       "      <td>1.374138</td>\n",
       "      <td>-0.380781</td>\n",
       "      <td>-0.995674</td>\n",
       "      <td>-0.399535</td>\n",
       "      <td>-1.362023</td>\n",
       "      <td>0.298863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>18.574557</td>\n",
       "      <td>0.245938</td>\n",
       "      <td>0.181293</td>\n",
       "      <td>-0.426479</td>\n",
       "      <td>-0.528526</td>\n",
       "      <td>-1.171791</td>\n",
       "      <td>-1.110293</td>\n",
       "      <td>-0.547967</td>\n",
       "      <td>-0.112603</td>\n",
       "      <td>-0.929282</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.417728</td>\n",
       "      <td>0.511963</td>\n",
       "      <td>-0.195478</td>\n",
       "      <td>-0.954541</td>\n",
       "      <td>-1.020919</td>\n",
       "      <td>-0.394498</td>\n",
       "      <td>0.386909</td>\n",
       "      <td>-0.355401</td>\n",
       "      <td>0.748051</td>\n",
       "      <td>0.120096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>318.199756</td>\n",
       "      <td>-0.113337</td>\n",
       "      <td>-0.417187</td>\n",
       "      <td>0.416573</td>\n",
       "      <td>0.092562</td>\n",
       "      <td>1.975764</td>\n",
       "      <td>-0.072938</td>\n",
       "      <td>-0.981171</td>\n",
       "      <td>-0.944513</td>\n",
       "      <td>1.277902</td>\n",
       "      <td>...</td>\n",
       "      <td>1.221550</td>\n",
       "      <td>0.420299</td>\n",
       "      <td>1.722744</td>\n",
       "      <td>0.318951</td>\n",
       "      <td>-1.817926</td>\n",
       "      <td>0.649493</td>\n",
       "      <td>1.121474</td>\n",
       "      <td>-0.504950</td>\n",
       "      <td>0.968843</td>\n",
       "      <td>-0.035614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>160.566505</td>\n",
       "      <td>-0.308673</td>\n",
       "      <td>-0.542890</td>\n",
       "      <td>1.151007</td>\n",
       "      <td>-0.357230</td>\n",
       "      <td>1.173319</td>\n",
       "      <td>-0.411164</td>\n",
       "      <td>1.049602</td>\n",
       "      <td>0.627835</td>\n",
       "      <td>-1.201848</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.233811</td>\n",
       "      <td>-0.751464</td>\n",
       "      <td>1.637849</td>\n",
       "      <td>0.995509</td>\n",
       "      <td>-0.216386</td>\n",
       "      <td>0.210938</td>\n",
       "      <td>0.359256</td>\n",
       "      <td>-0.800366</td>\n",
       "      <td>1.178297</td>\n",
       "      <td>-0.128824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target         0         1         2         3         4         5  \\\n",
       "0    -33.649387  0.485653  0.901637 -1.212651  0.139052  0.709391 -0.994617   \n",
       "1    120.201493 -0.893088 -0.071643  0.009755  1.347144  0.379104  0.093540   \n",
       "2   -141.708704  0.456587 -0.472096  1.606499  1.367399 -0.073593  0.981937   \n",
       "3   -184.095636 -0.765121 -0.938719 -0.854190  0.131660 -0.414848 -0.008027   \n",
       "4    246.179468 -0.382082  0.026456  2.268877  0.527533 -0.053195  1.566953   \n",
       "..          ...       ...       ...       ...       ...       ...       ...   \n",
       "495  -47.645625  0.270316 -0.414474 -0.998558 -0.315235 -0.528897  0.980720   \n",
       "496  -97.823612 -1.860027  0.694745  0.150319 -0.272093  0.974670 -0.352025   \n",
       "497   18.574557  0.245938  0.181293 -0.426479 -0.528526 -1.171791 -1.110293   \n",
       "498  318.199756 -0.113337 -0.417187  0.416573  0.092562  1.975764 -0.072938   \n",
       "499  160.566505 -0.308673 -0.542890  1.151007 -0.357230  1.173319 -0.411164   \n",
       "\n",
       "            6         7         8  ...        90        91        92  \\\n",
       "0    0.282911 -2.175066  0.853641  ... -0.287330  0.834672 -0.692276   \n",
       "1    0.585669 -1.109070 -0.617355  ...  1.024032  1.248742  0.383929   \n",
       "2   -0.002895  0.250887  0.215370  ... -0.540053 -1.386105 -1.124981   \n",
       "3   -0.988914 -1.434419  0.310836  ... -1.893789  0.059948 -0.887020   \n",
       "4   -1.114428 -0.545163  0.872815  ... -0.984395  0.361132  2.491734   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "495  0.484330 -0.183577 -0.929874  ... -1.110900 -0.214779  0.840373   \n",
       "496 -0.096521  0.307004 -0.686755  ...  0.536013  0.226708  0.220630   \n",
       "497 -0.547967 -0.112603 -0.929282  ... -0.417728  0.511963 -0.195478   \n",
       "498 -0.981171 -0.944513  1.277902  ...  1.221550  0.420299  1.722744   \n",
       "499  1.049602  0.627835 -1.201848  ... -0.233811 -0.751464  1.637849   \n",
       "\n",
       "           93        94        95        96        97        98        99  \n",
       "0    0.627274  1.149234  0.464482  1.593337  0.274752  2.152221  0.006960  \n",
       "1    0.456798  0.708521 -1.075047  0.428324 -0.803252 -0.114866  0.217488  \n",
       "2   -0.012272 -2.196037 -0.213129  1.151220  0.885248  1.226929 -0.100698  \n",
       "3   -0.700581 -0.671862  1.461510  2.365019 -1.583156 -1.046293 -1.532051  \n",
       "4   -1.471508 -1.187280 -1.700168  0.466667 -0.309938  0.253091 -0.076393  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "495  0.376225 -2.229612 -0.415535  0.857946 -0.408338  0.738022  0.470555  \n",
       "496 -0.734824  1.374138 -0.380781 -0.995674 -0.399535 -1.362023  0.298863  \n",
       "497 -0.954541 -1.020919 -0.394498  0.386909 -0.355401  0.748051  0.120096  \n",
       "498  0.318951 -1.817926  0.649493  1.121474 -0.504950  0.968843 -0.035614  \n",
       "499  0.995509 -0.216386  0.210938  0.359256 -0.800366  1.178297 -0.128824  \n",
       "\n",
       "[500 rows x 101 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv('~/shared/dataset1.csv')\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state=42)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information that you get with this dataset is the following:\n",
    "- This is a regression problem.\n",
    "- there are 500 cases\n",
    "- there are 100 features. That is a lot, and probably a large deal of those does not make sense. \n",
    "\n",
    "The question you get is: can you figure out what are the best features? And can you make a simple regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data to get familiar with it. First look at the shape, before you start visualizing. What would be feasable for these dimensions? (hint: dont try pairplot, but use `melt()`)\n",
    "\n",
    "Try to get an idea of the distribution of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, explore the correlations between the features. Pandas has a very convenient methode `.corr()` for that. Apply this method to the `data` pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cors = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c67012afb3c3ccac",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.sum(np.sum(cors)) == 100.50760431381602"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, get the target column (or row), they are equal.\n",
    "Remove the target-target correlation (which is always 1.0).\n",
    "Now plot the values of target correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a heatmap of the correlations, but use the variation where there is automatic clustering (`sns.clustermap`). This way we can spot clusters of features more easily. Handle the non-informative ones at the diagonal line to keep a good color contrast.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Click for hint</summary>\n",
    "    Hint: remove the correlations equal to 1 with <code>cors[cors==1] = 0</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a regularization method to make a simple linear model, and visualize the regularization weights to find the most usefull features. How many features would you select?\n",
    "\n",
    "<details>\n",
    "<summary>Click for hint 1</summary>\n",
    "    Hint: This is not a classification task. So do not blindly copy the code from the lesson. You can't use <code>SGDClassifier</code>. Also, is it necesarry for this dataset to use a scaler? If you are not sure, visualize the data.\n",
    "</details>  \n",
    "<details>\n",
    "<summary>Click for hint 2</summary>\n",
    "    Hint2: go to the \n",
    "    <a href=\"https://scikit-learn.org/stable/supervised_learning.html#supervised-learning\">sklearn documentation</a>. This is an overview of all supervised models. Look under 'Stochastic Gradient Descent' for a regressor.\n",
    "</details>    \n",
    "<details>\n",
    "<summary>Click for hint 3</summary>\n",
    "    Hint 3: use the option <code>penalty='elasticnet'</code> in the model you found with the help of hint 2.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    # your code here\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a gridsearch for the `l1_ratio` and `alpha` parameters, which will be used for the `elasticnet` you have set as the type of `penalty`in the model above. For the `l1_ratio`, you typically want to search the high values more then the low values. So use `l1_ratio = [.1, .5, .7, .9, .95, .99, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "l1_ratio = [.1, .5, .7, .9, .95, .99, 1]\n",
    "alphaList = [10**i for i in range(-4, 3)]\n",
    "\n",
    "param_grid = {\n",
    "    # your code here\n",
    "             } \n",
    "\n",
    "gridsearch = # your code here\n",
    "gridsearch.fit(X_train, y_train)\n",
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected outcome is `alpha=0.01` and `l1_ratio=1`. \n",
    "If not, set it to that value for the test below to pass.\n",
    "\n",
    "Now, we have a look at the weights. Compare this to the correlations. I hope you agree this give a much clearer picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = gridsearch.best_estimator_\n",
    "coefs = pipe.named_steps['sgd'].coef_.T\n",
    "plt.plot(coefs, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-38ce35dfbfa9b443",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.sum(coefs) == 421.32944561032184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in the second dataset. Information you get is:\n",
    "\n",
    "- this is a classification problem\n",
    "- the dataset has only two features\n",
    "- the question is to make a classification\n",
    "- the junior datascientist before you tried to make a linear classifier, but failed.\n",
    "\n",
    "Questions:\n",
    "\n",
    "- Can you find out why a linear model fails?\n",
    "- Can you create a simple model that does classify the data correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data. Make a train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv('~/shared/dataset2.csv')\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7)\n",
    "data.shape , X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is 2D, so visualize the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more ideas on colormaps, have a look at https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it is not obvious why a linear model will work out of the box, just by looking at the plot, try to draw a line for yourself that separates the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try fitting different classification models. Get some inspiration from the sklearn website that lists different [classifiers](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focus on the following things:\n",
    "\n",
    "- create pipes\n",
    "- add StandardScalers when usefull (e.g after adding polynomial features. Do you understand why that's a good idea?)\n",
    "- some suggestions: Polyfeatures, a scaler and SGDClassifier; A SVM with a kernel; Random Forests.\n",
    "- do a gridsearch to tune your models hyperparameters, and iterate this proces (tune parameters, zoom in, tune again, etc)\n",
    "- visualize the performace of the model under different parameter settings (heatmaps)\n",
    "- Are you able to visualize how the model makes a decision? E.g., how would the model classify any point in the feature space? Look at lesson 2, section 3 for inspiration on visualizing the inside of the model (contour plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hand in a to-the-point report. Make it so, that the reader can easily see: \n",
    "- which models did you try, and why?\n",
    "- how did you tune your hyperparameters?\n",
    "- what is the performance on the train-test set?\n",
    "- Add a confusion-matrix for the best model. Discuss precision-recall.\n",
    "- what is the best model, and why?\n",
    "- visualize the contours of the inside of your model.\n",
    "\n",
    "Deadline for handing this in is sunday 23:59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
