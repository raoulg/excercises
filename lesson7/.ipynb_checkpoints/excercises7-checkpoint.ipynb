{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXym_zab5n6O"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, Input, MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the data. This are two classes. Every class has it's own file. Our task it to discern clickbait titles from non-clickbait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '~/shared'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbnCcr5Y6gkb"
   },
   "outputs": [],
   "source": [
    "file1 = os.path.join(os.path.expanduser(datadir), 'clickbait_data.txt')\n",
    "click = pd.read_csv(file1, header=None, delimiter='\\n', names=['text'])\n",
    "click['label'] = 1\n",
    "\n",
    "file2 = os.path.join(os.path.expanduser(datadir), 'non_clickbait_data.txt')\n",
    "noclick = pd.read_csv(file2, header=None, delimiter='\\n',  names=['text'])\n",
    "noclick['label'] = 0\n",
    "\n",
    "data = pd.concat([click, noclick], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the size of all observations, and define a batchsize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = len(data)\n",
    "BATCH = 32\n",
    "SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `tf.data.Dataset`. You can feed it the `text` and `label` columns as a single tuple, eg `(data['text'], data['label'])`. After that, shuffle the dataset with `buffer_size=SIZE` and make batch the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtwRieAi42LQ"
   },
   "outputs": [],
   "source": [
    "ds = \n",
    "ds = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check one batch visually with `take(1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GI0kBWBu46_q",
    "outputId": "d2fbc5f9-c884-411f-e4c6-35c608aa63aa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a train and test set with a 80% split. Remember that your dataset is batched, so you should use `SIZE/BATCH` as the total amount of items.\n",
    "\n",
    "Use `.take()` and `.skip()` to take the first n observations, and then skip the first n observations to create your sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doVXNEMqARyj"
   },
   "outputs": [],
   "source": [
    "train_n = \n",
    "train_ds = \n",
    "val_ds = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `.prefetch()` with `tf.data.experimental.AUTOTUNE` to prefetch the data. This speeds up performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkUaFOMj5n6T"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = \n",
    "train_ds = \n",
    "val_ds = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhXrkg495n6U"
   },
   "source": [
    "# Clean and preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create preprocessing where you set everything to lowercase and remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_s2elK1_BUCw",
    "outputId": "c30b0ffe-3f8b-40d5-d64f-c0d3f5814e0d"
   },
   "outputs": [],
   "source": [
    "punctuation = '[%s]' % string.punctuation\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    x = \n",
    "    x = \n",
    "    return x\n",
    "\n",
    "custom_standardization(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creater a `TextVectorization` layer. Pick a `vocab_size` and `sequence_length`, and add your `custom_standardization`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3joqFAu5n6U",
    "outputId": "bfd5c68d-f556-4e41-e6d6-e38d088765f3"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Pick a vocabulary size and number of words in a sequence.\n",
    "vocab_size = \n",
    "sequence_length =\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to \n",
    "# integers. Note that the layer uses the custom standardization defined above. \n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=\n",
    "    max_tokens=\n",
    "    output_mode='int',\n",
    "    output_sequence_length= \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use `.adapt` to create the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a model with the following architecture:\n",
    "\n",
    "- an input layer with `shape=[1]` and `dtype=tf.string`\n",
    "- your vectorize_layer\n",
    "- an `Embedding` layer. Set the embedding to 100.\n",
    "- `GlobalAveragePooling1D`\n",
    "- one `Dense` layer, with 64 units and `relu`\n",
    "- a final `Dense` layer with one unit and a `sigmoid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuO810Rj5n6V"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, Dropout, GlobalAveragePooling1D\n",
    "model = Sequential([\n",
    "\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A0gHDX5j5n6V",
    "outputId": "e3600d8d-346c-4db9-b003-4965844e5990"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "model.compile(optimizer=Adam(lr=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy', Precision(name='precision'), Recall(name='recall')])\n",
    "\n",
    "\n",
    "model.fit(train_ds, epochs=3, validation_data=val_ds, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5Ui-Z1yDSg-"
   },
   "source": [
    "# Rotten Tomatoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9NFzXYNDUKc"
   },
   "outputs": [],
   "source": [
    "file = os.path.join(os.path.expanduser(datadir), 'rotten_tomatoes_movies.csv')\n",
    "\n",
    "data = pd.read_csv(file)\n",
    "df = data[['movie_info', 'genres']]\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['movie_info'][0], df['genres'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see small descriptions of a movie. We also have a list of genres. Actually, there are really a lot different types of genres, 38 in total. Let's start our model with a small selection of the genres. We use the `re` library to find certain (parts of) a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5TSzfGBXu-T"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "df['select'] = df.genres.apply(lambda x: re.findall('Science Fiction|Romance|Comedy|Action|Art', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now have a multiclass, multilabel classification problem. Some movies will fall in more then one category! First, let's create binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2lsUNl0VaKX_",
    "outputId": "427a8608-0a72-4d7a-b091-f1468df62012"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "X = df['movie_info']\n",
    "y = mlb.fit_transform(df['select'])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2rshaDM7au5E",
    "outputId": "6c0441d2-af4f-412b-9822-77fcebfd8d16"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's only keep the movies that fall at least in one of the categories we picked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyTnKc1DaTdb",
    "outputId": "05f17656-74a8-4f86-81b9-af984b08eaea"
   },
   "outputs": [],
   "source": [
    "keep = np.sum(y, axis=1) != 0\n",
    "X = X[keep]\n",
    "y = y[keep]\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we lost about 6000 movies, but we still have enough to make a model. If you want to experiment, you can add more categories and see if you can still get good results. But first, let us visualize the distribution of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "t9MOshXJQVBM",
    "outputId": "28c850c3-cf1c-4ca0-f787-2905bd2b80f6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "distribution = np.mean(y, axis=0)\n",
    "plt.bar(range(len(distribution)), distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's not a uniform distribution. But we have all categories covered. So while we might want to add precision and recall to be sure, this will probably work. We might get into problems if we had one category really under-represented (eg 0.01%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2poYT2DDjdm",
    "outputId": "5d117c3e-ad38-4199-d02f-52c80dc8e0e2"
   },
   "outputs": [],
   "source": [
    "SIZE = len(X)\n",
    "BATCH = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5edYr6DYSlgd",
    "outputId": "e1ce473a-810c-4e7e-f433-ed7986b8f01b"
   },
   "outputs": [],
   "source": [
    "CLASSES = y.shape[1]\n",
    "CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before:\n",
    "- generate datasets from tensor slices\n",
    "- shuffle and batch\n",
    "- pick a train-test ratio\n",
    "- create sets with `take` and `skip`\n",
    "- prefetch with AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayf2p1nsE0wE"
   },
   "outputs": [],
   "source": [
    "ds = \n",
    "ds = \n",
    "\n",
    "train_n = \n",
    "train_ds = \n",
    "val_ds = \n",
    "\n",
    "AUTOTUNE = \n",
    "train_ds = \n",
    "val_ds = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oOyo5yrYGTrh",
    "outputId": "38d928f1-4762-4dca-ca49-31b4f507c0fa"
   },
   "outputs": [],
   "source": [
    "for x, y in train_ds.take(1):\n",
    "  print(x)\n",
    "  print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a `vocab_size` and `sequence_length` and create a `TextVectorization` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3uymEJ7XHmHB"
   },
   "outputs": [],
   "source": [
    "vocab_size = \n",
    "sequence_length = \n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to \n",
    "# integers. Note that the layer uses the custom standardization defined above. \n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `adapt` to get the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWRs5ztLIkSX"
   },
   "outputs": [],
   "source": [
    "text_ds = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse the model you tried above. But instead of `sigmoid`, don't use any activation in the last `Dense` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJ5i58CzIo9r"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "\n",
    "    \n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might remember from the previous lesson, that you should use `from_logits=True` if you ommit the activation in the last layer. To refresh your understanding of how this works, try the example below.\n",
    "you find:\n",
    "- a three class label for a batch of 2, that is also multilabel multiclass\n",
    "- logit prediction for a batch of 2\n",
    "\n",
    "Try to increase and decrease the predictions by modifying the numbers below. First, decide if you want to get the loss up  or down. Then, modify the prediction. Check if you understand whats happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YAHnVc0TKpqu",
    "outputId": "9ab10db1-c145-4221-dce9-ecfe2fb7bda8"
   },
   "outputs": [],
   "source": [
    "y_true = [[1, 0, 1], [0, 0, 1]]\n",
    "y_pred = [[5.0, -10.0, 5], [-5.0, -10, 20]]\n",
    "loss = tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=True)\n",
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcBD000-JPXH"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7F1hyjMcKVHT",
    "outputId": "e8a52050-6715-4673-9adb-1ddcdfba8912",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds, \n",
    "    epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the model. We grab the first two texts from our validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, label in val_ds.take(1):\n",
    "    print(text[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(text[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the original label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb.inverse_transform(label[:2].numpy())\n",
    "# note that \"Science Fiction\" is actually \"Science Fiction & Fantasy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check for ourselves, we can use `inverse_tranform` from the `mlb`. It is interesting how to model actually adds something to the original binary labels. While both examples migth predict correctly a movie to be comedy, the model tells us that it is much more clear from the text that the second one is a comedy (eg with values of 4 versus 12). Also, for a single movie, it can tell you which labels seems to be more likely or dominant. Try for yourself some more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create an architecture with a RNN. use the following:\n",
    "- an `Input` layer\n",
    "- your `vectorize_layer`\n",
    "- an `Embedding` layer\n",
    "- a type of RNN. Try `GRU` first, with 16 units.\n",
    "- A final `Dense` layer, without an activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hpwh3BixLToF",
    "outputId": "b4a6eeb9-bf37-46eb-886c-220d5768fc32"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "\n",
    "model = Sequential([\n",
    "\n",
    "    \n",
    "    \n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efVp1D-MNujQ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(optimizer=Adam(lr=1e-3),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q5SSLpYgOzFV",
    "outputId": "10ef28cf-294d-4977-e5f3-c6d82bb39e51"
   },
   "outputs": [],
   "source": [
    "model.fit(train_ds,\n",
    "          epochs=20,\n",
    "          validation_data=val_ds,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "excercises7-solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
